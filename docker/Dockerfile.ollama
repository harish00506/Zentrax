# Ollama LLM Container for Zentrax
FROM ollama/ollama:latest

# Set environment variables
ENV OLLAMA_HOST=0.0.0.0
ENV OLLAMA_MODELS=/root/.ollama/models

# Expose the Ollama API port
EXPOSE 11434

# Create startup script
RUN echo '#!/bin/bash\n\
echo "Starting Ollama server..."\n\
ollama serve &\n\
sleep 5\n\
echo "Pulling SmolLM2 model..."\n\
ollama pull smollm2\n\
echo "Ollama ready with SmolLM2!"\n\
wait' > /start.sh && chmod +x /start.sh

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:11434/api/tags || exit 1

# Start Ollama and pull model
CMD ["/start.sh"]
